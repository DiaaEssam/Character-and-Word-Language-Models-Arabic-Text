{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8332197,"sourceType":"datasetVersion","datasetId":4947747}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing libraries","metadata":{"id":"s31Y6zF8XvhS"}},{"cell_type":"code","source":"! pip install wikipedia\n! pip install reportlab\n! pip install arabic_reshaper\n! pip install python-bidi","metadata":{"id":"cT_zmy-aXvhY","executionInfo":{"status":"ok","timestamp":1714940221961,"user_tz":-180,"elapsed":12371,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"outputId":"774a25df-1601-4361-df60-abcce7727e7a","execution":{"iopub.status.busy":"2024-05-06T09:02:25.634776Z","iopub.execute_input":"2024-05-06T09:02:25.635428Z","iopub.status.idle":"2024-05-06T09:03:13.811434Z","shell.execute_reply.started":"2024-05-06T09:02:25.635396Z","shell.execute_reply":"2024-05-06T09:03:13.810178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{"id":"8FPZY5RGXvhc"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport wikipedia\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Image, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet \nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.pdfbase.ttfonts import TTFont\nfrom reportlab.lib.styles import ParagraphStyle\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\npdfmetrics.registerFont(TTFont('Arabic', '/kaggle/input/pdf-font/arfonts-bahij-tanseek-pro.ttf'))","metadata":{"id":"C4pDC458Xvhd","executionInfo":{"status":"ok","timestamp":1714940874543,"user_tz":-180,"elapsed":354,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.814198Z","iopub.execute_input":"2024-05-06T09:03:13.814526Z","iopub.status.idle":"2024-05-06T09:03:13.838325Z","shell.execute_reply.started":"2024-05-06T09:03:13.814498Z","shell.execute_reply":"2024-05-06T09:03:13.837437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function  to Generate documents","metadata":{"id":"d4dPgGSjXvhe"}},{"cell_type":"code","source":"def get_documents():\n    # Set the language to Arabic\n    wikipedia.set_lang(\"ar\")\n\n    # Search for the term \"صلاح الدين\" (Saladin) on Wikipedia\n    results = wikipedia.search(\"صلاح الدين\")\n\n    # Choose the most relevant page from the search results\n    # Assumes the first result is the most relevant\n    page = wikipedia.page(results[0])\n\n    # Extract the content of the selected Wikipedia page\n    # Limit the extracted content to the first 100,000 characters\n    text = page.content[:100000]\n\n    # Print the first 1,000 characters of the extracted content\n    print(text[:1000] + '\\n')\n\n    # Return the extracted content\n    return text","metadata":{"id":"Nr60QEC8Xvhe","executionInfo":{"status":"ok","timestamp":1714943129846,"user_tz":-180,"elapsed":657,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.839656Z","iopub.execute_input":"2024-05-06T09:03:13.840001Z","iopub.status.idle":"2024-05-06T09:03:13.845862Z","shell.execute_reply.started":"2024-05-06T09:03:13.839975Z","shell.execute_reply":"2024-05-06T09:03:13.844898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to build chars dictionaries","metadata":{}},{"cell_type":"code","source":"def get_chars_dictionaries(text):\n    # Create a sorted list of unique characters in the text\n    chars = sorted(list(set(text)))\n\n    # Create a dictionary mapping each character to its index\n    # char_indices maps characters to their corresponding indices\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n\n    # Create a dictionary mapping each index to its corresponding character\n    # indices_char maps indices to their corresponding characters\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n\n    # Return the list of unique characters and the two dictionaries\n    return chars, char_indices, indices_char","metadata":{"id":"oVM2VwziXvhf","executionInfo":{"status":"ok","timestamp":1714943129846,"user_tz":-180,"elapsed":8,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.846983Z","iopub.execute_input":"2024-05-06T09:03:13.847274Z","iopub.status.idle":"2024-05-06T09:03:13.857294Z","shell.execute_reply.started":"2024-05-06T09:03:13.847251Z","shell.execute_reply":"2024-05-06T09:03:13.856442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to prepare data as X & y","metadata":{}},{"cell_type":"code","source":"def prepare_data(text, window=40):\n    X = []  # List to store input sequences\n    y = []  # List to store corresponding target characters\n\n    # Iterate over sentences in the text\n    for sentence in text.split(\".\"):\n        # Remove leading/trailing whitespace from the sentence\n        sentence = sentence.strip()\n\n        # Skip empty sentences\n        if len(sentence) == 0:\n            continue\n\n        # Iterate over the characters in the sentence\n        for i in range(0, len(sentence) - window, 1):\n            # Extract a substring of length 'window' as the input sequence\n            X.append(sentence[i : i + window])\n\n            # Extract the character that follows the input sequence as the target\n            y.append(sentence[i + window])\n\n    # Return the input sequences (X) and target characters (y)\n    return X, y","metadata":{"id":"s6AZdPpPXvhg","executionInfo":{"status":"ok","timestamp":1714943129846,"user_tz":-180,"elapsed":4,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.860423Z","iopub.execute_input":"2024-05-06T09:03:13.861065Z","iopub.status.idle":"2024-05-06T09:03:13.870536Z","shell.execute_reply.started":"2024-05-06T09:03:13.861041Z","shell.execute_reply":"2024-05-06T09:03:13.869645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to Convert characters to One Hot representation","metadata":{}},{"cell_type":"code","source":"def convert_to_OHV(X, y, maxlen, chars, char_indices):\n    # Create a 3D numpy array to store the one-hot vector representation of input sequences\n    X_OHV = np.zeros((len(X), maxlen, len(chars)), dtype=bool)\n    \n    # Create a 2D numpy array to store the one-hot vector representation of target characters\n    y_OHV = np.zeros((len(y), len(chars)), dtype=bool)\n\n    # Iterate over input sequences (X) and target characters (y)\n    for i, sentence in enumerate(X):\n        # Iterate over characters in the input sequence\n        for j, char in enumerate(sentence):\n            # Set the corresponding element in X_OHV to 1 based on the character index\n            X_OHV[i, j, char_indices[char]] = 1\n        \n        # Set the corresponding element in y_OHV to 1 based on the target character index\n        y_OHV[i, char_indices[y[i]]] = 1\n    \n    # Return the one-hot vector representations of input sequences and target characters\n    return X_OHV, y_OHV","metadata":{"id":"9YqGECm2Xvhh","executionInfo":{"status":"ok","timestamp":1714943129846,"user_tz":-180,"elapsed":3,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.871746Z","iopub.execute_input":"2024-05-06T09:03:13.872092Z","iopub.status.idle":"2024-05-06T09:03:13.881076Z","shell.execute_reply.started":"2024-05-06T09:03:13.872060Z","shell.execute_reply":"2024-05-06T09:03:13.880210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to build models","metadata":{}},{"cell_type":"code","source":"def build_char_model(model_parameters, model_number, maxlen, chars, h=100, drop_rate=0.1):\n    # Create a Sequential model\n    model = tf.keras.Sequential([\n        # Add a bidirectional SimpleRNN layer with 'h' hidden units, and input shape specified\n        tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(h, return_sequences=True), input_shape=(maxlen, len(chars))),\n        # Add another bidirectional SimpleRNN layer with double the hidden units of the previous RNN layer\n        tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(h * 2)),\n        # Add a dropout layer to prevent overfitting, using the specified dropout rate\n        tf.keras.layers.Dropout(drop_rate),\n        # Add a Dense output layer with a softmax activation function to output a probability distribution over the characters\n        tf.keras.layers.Dense(len(chars), activation='softmax')\n    ])\n\n    # Compile the model with categorical crossentropy as the loss function and the Adam optimizer\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Save the model parameters (hidden units, dropout rate) for future reference using the model number as the key\n    model_parameters[model_number] = [h, drop_rate]\n\n    # Print the details of the model including the model number and parameters\n    print(f\"\\nModel{model_number}\")\n    print(f\"Model Parameters: No. Hidden Neurons = {model_parameters[model_number][0]}, Drop Rate = {model_parameters[model_number][1]}\\n\")\n\n    # Plot the model architecture (plot_model function needs to be defined elsewhere or imported)\n    # This might save a visual plot of the model or display it using a graphical frontend\n    plot_model(model, model_number)\n\n    # Return the constructed model\n    return model","metadata":{"id":"v5p-GuqBXvhi","executionInfo":{"status":"ok","timestamp":1714943130448,"user_tz":-180,"elapsed":4,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.882246Z","iopub.execute_input":"2024-05-06T09:03:13.882526Z","iopub.status.idle":"2024-05-06T09:03:13.895515Z","shell.execute_reply.started":"2024-05-06T09:03:13.882504Z","shell.execute_reply":"2024-05-06T09:03:13.894574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to generate characters using the trained language model","metadata":{}},{"cell_type":"code","source":"def generate_char_text(model, text, maxlen, chars, char_indices, indices_char, temperature=0.5):\n    # Randomly select a starting index in the text\n    start_index = np.random.randint(0, len(text) - maxlen - 1)\n    \n    # Extract the seed text starting from the selected index\n    generated_text = text[start_index: start_index + maxlen]\n    total_generated_text = text[start_index: start_index + maxlen]\n    \n    # Print the seed text\n    print(\"Seed:\", generated_text)\n\n    # Generate 400 characters\n    for i in range(400):\n        # Create a one-hot encoded representation of the generated text\n        sampled = np.zeros((1, maxlen, len(chars)))\n        for t, char in enumerate(generated_text):\n            sampled[0, t, char_indices[char]] = 1.\n\n        # Predict the next character probabilities using the model\n        preds = model.predict(sampled, verbose=0)[0]\n        \n        # Adjust the predicted probabilities based on the temperature\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n\n        # Sample the next character index based on the adjusted probabilities\n        next_index = np.random.choice(len(chars), p=preds)\n        \n        # Convert the sampled index to the corresponding character\n        next_char = indices_char[next_index]\n\n        # Append the next character to the generated text\n        generated_text += next_char\n        \n        # Remove the first character from the generated text to maintain the maxlen\n        generated_text = generated_text[1:]\n        \n        # Append the next character to the total generated text\n        total_generated_text += next_char\n    \n    # Return the total generated text\n    return total_generated_text","metadata":{"id":"M_iez2ENXvhj","executionInfo":{"status":"ok","timestamp":1714943130448,"user_tz":-180,"elapsed":3,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.896925Z","iopub.execute_input":"2024-05-06T09:03:13.897569Z","iopub.status.idle":"2024-05-06T09:03:13.910692Z","shell.execute_reply.started":"2024-05-06T09:03:13.897544Z","shell.execute_reply":"2024-05-06T09:03:13.909871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to generate PDF containing the results","metadata":{}},{"cell_type":"code","source":"def generate_results_pdf(model_parameters, generated_text):\n    \n    # Create a custom style for Arabic text\n    arabic_style = ParagraphStyle(name='ArabicStyle', fontName='Arabic', fontSize=12)\n    \n    # Create a PDF document\n    doc = SimpleDocTemplate(\"Results.pdf\", pagesize=letter)\n\n    # Define the styles for the document\n    styles = getSampleStyleSheet()\n\n    # Create a list to store the PDF components\n    pdf_components = []\n\n    # Iterate over each model parameter in the dictionary\n    for i, model_parameter in model_parameters.items():\n\n        # Create a heading for the model\n        heading1 = Paragraph(f\"Model {i}:\", styles[\"Heading1\"])\n\n        # Create a paragraph with the model parameters\n        text = f\"Model Parameters: Embedding dimension = {model_parameters[i][0]}, No. Hidden Neurons = {model_parameters[i][1]}\" if i>3 else f\"Model Parameters: No. Hidden Neurons = {model_parameters[i][0]}, Drop Rate = {model_parameters[i][1]}\"\n        para1 = Paragraph(text, styles[\"Normal\"])\n\n        # Load and add the model plot image to the PDF components\n        img = Image(f\"/kaggle/working/model_plot{i}.png\", width=300, height=500)\n        pdf_components += [heading1] + [Spacer(1, 20)] + [para1] + [Spacer(1, 20)] + [img]\n\n        # Load and add the model history plot image to the PDF components\n        acc = Image(f\"/kaggle/working/Model_history_{i}.png\", width=300, height=300)\n        print(generated_text[i])\n        pdf_components += [Spacer(1, 20)] + [acc] + [Spacer(1, 20)] + [Paragraph(\"Generated Text:\", styles[\"Heading1\"])] + [Spacer(1, 20)] +  [Paragraph(get_display(arabic_reshaper.reshape(generated_text[i])), arabic_style)] + [Spacer(1, 20)]\n\n    # Create a heading for the Conclusion\n    heading2 = Paragraph(\"Conclusion:\")\n\n    # Conclusion paragraph\n    text = \"The first model seems to outperform the other two models based on the accuracy and generated text.\" if list(model_parameters.keys())[2] < 4 else \"The third model seems to outperform the other two models based on the accuracy and generated text.\"\n    para2 = Paragraph(text, styles[\"Normal\"])\n\n    # Build the PDF document with the components\n    return pdf_components + [heading2] + [para2] + [Spacer(1, 20)] , doc","metadata":{"id":"mwY3N3nMXvhj","executionInfo":{"status":"ok","timestamp":1714943130449,"user_tz":-180,"elapsed":4,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.911873Z","iopub.execute_input":"2024-05-06T09:03:13.912125Z","iopub.status.idle":"2024-05-06T09:03:13.927488Z","shell.execute_reply.started":"2024-05-06T09:03:13.912103Z","shell.execute_reply":"2024-05-06T09:03:13.926633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to plot model history","metadata":{}},{"cell_type":"code","source":"def plot_history(history, save_path=None):\n    # Plot training accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.title(f'Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train'], loc='upper left')\n\n    # Save the plot if save_path is provided\n    if save_path:\n        plt.savefig(save_path)\n\n    plt.show()","metadata":{"id":"1wdi7K_mbPTl","executionInfo":{"status":"ok","timestamp":1714943130449,"user_tz":-180,"elapsed":3,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.928587Z","iopub.execute_input":"2024-05-06T09:03:13.929588Z","iopub.status.idle":"2024-05-06T09:03:13.939635Z","shell.execute_reply.started":"2024-05-06T09:03:13.929562Z","shell.execute_reply":"2024-05-06T09:03:13.938879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to plot model architecture","metadata":{}},{"cell_type":"code","source":"def plot_model(model, model_number):\n\n    # Plot the model architecture using plot_model function from Keras\n    tf.keras.utils.plot_model(model, to_file=f'model_plot{model_number}.png', show_shapes=True, show_layer_names=True)\n\n    # Load the image generated by plot_model function\n    img = mpimg.imread(f'/kaggle/working/model_plot{model_number}.png')\n\n    # Set the figure size (width, height) in inches for the plot\n    plt.figure(figsize=(10, 11))\n\n    # Plot the image without axis labels\n    plt.imshow(img)\n    plt.axis('off')\n    plt.show()","metadata":{"id":"KHieF0sWcFBa","executionInfo":{"status":"ok","timestamp":1714943130878,"user_tz":-180,"elapsed":4,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.940719Z","iopub.execute_input":"2024-05-06T09:03:13.941188Z","iopub.status.idle":"2024-05-06T09:03:13.952547Z","shell.execute_reply.started":"2024-05-06T09:03:13.941164Z","shell.execute_reply":"2024-05-06T09:03:13.951757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to fit LMs","metadata":{}},{"cell_type":"code","source":"def fit_char_models(models, X_OHV, y_OHV, text, maxlen, chars, char_indices, indices_char):\n\n    generated_text = {}\n    # Iterate over each model in the list of models\n    for i, model in enumerate(models):\n\n        # Print the model number for clarity\n        print(f\"\\nModel{i+1}:\")\n\n        # Fit the model on the training data\n        history = model.fit(X_OHV, y_OHV, batch_size=64, epochs=25)\n\n        # Print a blank line for clarity\n        print(\"\\n\")\n\n        # Plot the training history for the current model\n        plot_history(history, save_path=f\"Model_history_{i+1}.png\")\n        seed_text = \"صلاح الدين\"\n        generated_text[i+1] = generate_char_text(model, text, maxlen, chars, char_indices, indices_char)\n    return generated_text","metadata":{"id":"7ouai_mXXvhk","executionInfo":{"status":"ok","timestamp":1714943131238,"user_tz":-180,"elapsed":2,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:13:52.038304Z","iopub.execute_input":"2024-05-06T09:13:52.038645Z","iopub.status.idle":"2024-05-06T09:13:52.045238Z","shell.execute_reply.started":"2024-05-06T09:13:52.038620Z","shell.execute_reply":"2024-05-06T09:13:52.044338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to train character level LMs","metadata":{}},{"cell_type":"code","source":"def train_char_LMs():\n    # Set the maximum length of input sequences\n    maxlen = 40\n    \n    # Get the input text documents\n    text = get_documents()\n    \n    # Get the character dictionaries\n    chars, char_indices, indices_char = get_chars_dictionaries(text)\n    \n    # Prepare the input sequences and target characters\n    X, y = prepare_data(text, window=maxlen)\n    \n    # Convert the input sequences and target characters to one-hot vector representations\n    X_OHV, y_OHV = convert_to_OHV(X, y, maxlen, chars, char_indices)\n    \n    # Initialize a dictionary to store model parameters\n    model_parameters = {}\n    \n    # Define the list of models to train\n    models = [\n        build_char_model(model_parameters, 1, maxlen, chars),\n        build_char_model(model_parameters, 2, maxlen, chars, h=150, drop_rate=0.5),\n        build_char_model(model_parameters, 3, maxlen, chars, h=200, drop_rate=0.7),\n    ]\n    \n    # Train the models and generate text\n    generated_text = fit_char_models(models, X_OHV, y_OHV, text, maxlen, chars, char_indices, indices_char)\n    \n    # Generate the results PDF\n    pdf_components, doc = generate_results_pdf(model_parameters, generated_text)\n    \n    # Return the PDF components and the document object\n    return pdf_components, doc","metadata":{"id":"BtaaD1J0Xvhl","executionInfo":{"status":"ok","timestamp":1714943131619,"user_tz":-180,"elapsed":2,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:03:13.964909Z","iopub.execute_input":"2024-05-06T09:03:13.966542Z","iopub.status.idle":"2024-05-06T09:03:13.974429Z","shell.execute_reply.started":"2024-05-06T09:03:13.966516Z","shell.execute_reply":"2024-05-06T09:03:13.973548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdf_components, doc = train_char_LMs()","metadata":{"id":"XRnrYTFdZZl9","executionInfo":{"status":"ok","timestamp":1714943269931,"user_tz":-180,"elapsed":137849,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"outputId":"df5624fc-01c1-4f2c-f347-133f20c08bc7","execution":{"iopub.status.busy":"2024-05-06T09:13:57.241367Z","iopub.execute_input":"2024-05-06T09:13:57.242227Z","iopub.status.idle":"2024-05-06T09:38:53.375185Z","shell.execute_reply.started":"2024-05-06T09:13:57.242192Z","shell.execute_reply":"2024-05-06T09:38:53.374254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to preprocess data for word level LM","metadata":{}},{"cell_type":"code","source":"def preprocess_data(text):\n    # Split the text into sentences using the Arabic comma (،) as the delimiter\n    sentences = text.split(\"،\")\n\n    # Create a Tokenizer object from the Keras preprocessing utilities\n    # Set the maximum number of words to keep in the word index to 15000\n    tokenizer = Tokenizer(num_words=15000)\n\n    # Build the word index from the sentences\n    tokenizer.fit_on_texts(sentences)\n\n    # Get the total number of unique words in the word index (plus 1 for the reserved 0 index)\n    total_words = len(tokenizer.word_index) + 1\n\n    # Return the total number of words, the tokenizer object, and the list of sentences\n    return total_words, tokenizer, sentences","metadata":{"id":"vjL3lqlRXvhl","executionInfo":{"status":"ok","timestamp":1714945754594,"user_tz":-180,"elapsed":425,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:38:53.377946Z","iopub.execute_input":"2024-05-06T09:38:53.378239Z","iopub.status.idle":"2024-05-06T09:38:53.384964Z","shell.execute_reply.started":"2024-05-06T09:38:53.378213Z","shell.execute_reply":"2024-05-06T09:38:53.382996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to prepare data as X & y","metadata":{}},{"cell_type":"code","source":"def prepare_training_data(sentences, tokenizer, total_words):\n    # Initialize an empty list to store the input sequences\n    input_sequences = []\n\n    # Convert the sentences to sequences of integer word indices\n    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n\n    # Create input sequences by iterating over each tokenized sentence\n    for sentence in tokenized_sentences:\n        for i in range(1, len(sentence)):\n            # Create an n-gram sequence from the start of the sentence to the current position\n            n_gram_sequence = sentence[: i + 1]\n            # Append the n-gram sequence to the list of input sequences\n            input_sequences.append(n_gram_sequence)\n\n    # Find the maximum length of the input sequences\n    maxlen = max([len(x) for x in input_sequences])\n\n    # Pad the input sequences to the maximum length\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding='pre'))\n\n    # Split the input sequences into input data (Xs) and labels (labels)\n    Xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n\n    # One-hot encode the labels\n    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n\n    # Return the input data, one-hot encoded labels, and the maximum sequence length\n    return Xs, ys, maxlen","metadata":{"id":"InnAHOhgXvhm","executionInfo":{"status":"ok","timestamp":1714945755086,"user_tz":-180,"elapsed":4,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:38:53.405568Z","iopub.execute_input":"2024-05-06T09:38:53.405818Z","iopub.status.idle":"2024-05-06T09:38:53.413163Z","shell.execute_reply.started":"2024-05-06T09:38:53.405795Z","shell.execute_reply":"2024-05-06T09:38:53.412318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to build word level LMs","metadata":{}},{"cell_type":"code","source":"def build_word_model(model_number, model_parameters, maxlen, vocab_length, d=20, h=100):\n\n    # Define the input layer\n    i = tf.keras.layers.Input((maxlen-1,))\n\n    # Add an embedding layer\n    x = tf.keras.layers.Embedding(vocab_length, d)(i)\n\n    # Add Hidden Layer 1 - RNN Layer\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(h, return_sequences=True)) (x)\n    \n    # Add Hidden Layer 2 - RNN Layer\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(h * 2)) (x)\n\n    # Add a Dropout layer for regularization\n    x = tf.keras.layers.Dropout(0.1)(x)\n\n    # Add a Dense layer with sigmoid activation for binary classification\n    x = tf.keras.layers.Dense(vocab_length, activation='softmax')(x)\n\n    # Create the model\n    model = tf.keras.models.Model(i, x)\n\n    # Compile the model with binary cross-entropy loss and Adam optimizer\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # Save the model parameters for future reference\n    model_parameters[model_number] = [d, h]\n\n    # Print the model details\n    print(f\"\\nModel{model_number}\")\n    print(f\"Model Parameters: Embedding dimension = {model_parameters[model_number][0]}, No. Hidden Neurons = {model_parameters[model_number][1]}\\n\")\n\n    # Plot the model architecture\n    plot_model(model, model_number)\n\n    # Return the model\n    return model","metadata":{"id":"RVCa0KfTXvhm","executionInfo":{"status":"ok","timestamp":1714945756171,"user_tz":-180,"elapsed":6,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:45:21.575331Z","iopub.execute_input":"2024-05-06T09:45:21.576139Z","iopub.status.idle":"2024-05-06T09:45:21.587816Z","shell.execute_reply.started":"2024-05-06T09:45:21.576095Z","shell.execute_reply":"2024-05-06T09:45:21.586753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to generate words using the trained LM","metadata":{}},{"cell_type":"code","source":"def generate_word_text(seed_text, next_words, model, max_sequence_len, tokenizer, temperature=0.5):\n    # Generate the specified number of words\n    for _ in range(next_words):\n        # Convert the seed text to a sequence of integer word indices\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        \n        # Pad the sequence to the maximum sequence length\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len)\n        \n        # Adjust the temperature to introduce randomness\n        preds = model.predict(token_list, verbose=0)[0]\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        \n        # Sample the next word based on the adjusted probabilities\n        predicted_index = np.random.choice(len(preds), p=preds)\n        \n        # Find the word corresponding to the predicted index\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted_index:\n                output_word = word\n                break\n                \n        # Append the new word to the seed text\n        seed_text += \" \" + output_word\n\n    # Capitalize the first letter of the generated text and return it\n    return seed_text.title()","metadata":{"id":"XrslPJhLXvhn","executionInfo":{"status":"ok","timestamp":1714945756172,"user_tz":-180,"elapsed":6,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:45:26.449694Z","iopub.execute_input":"2024-05-06T09:45:26.450064Z","iopub.status.idle":"2024-05-06T09:45:26.458399Z","shell.execute_reply.started":"2024-05-06T09:45:26.450036Z","shell.execute_reply":"2024-05-06T09:45:26.457340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to fit word LMs","metadata":{}},{"cell_type":"code","source":"def fit_word_models(models, Xs, ys, maxlen, tokenizer):\n\n    generated_text = {}\n    # Iterate over each model in the list of models\n    for i, model in enumerate(models):\n\n        # Print the model number for clarity\n        print(f\"\\nModel{i+1}:\")\n\n        # Fit the model on the training data\n        history = model.fit(Xs, ys, batch_size=64, epochs=100)\n\n        # Print a blank line for clarity\n        print(\"\\n\")\n\n        # Plot the training and validation history for the current model\n        plot_history(history, save_path=f\"Model_history_{i+4}.png\")\n\n        seed_text = \"صلاح الدين\"\n        generated_text[i+4] = generate_word_text(seed_text, 100, model, maxlen-1, tokenizer)\n    return generated_text","metadata":{"id":"Qmzkp5EGXvhn","executionInfo":{"status":"ok","timestamp":1714946205919,"user_tz":-180,"elapsed":495,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:45:31.183758Z","iopub.execute_input":"2024-05-06T09:45:31.184119Z","iopub.status.idle":"2024-05-06T09:45:31.190974Z","shell.execute_reply.started":"2024-05-06T09:45:31.184092Z","shell.execute_reply":"2024-05-06T09:45:31.189816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to train word level LMs","metadata":{}},{"cell_type":"code","source":"def train_word_LLMs():\n    # Get the text data\n    text = get_documents()\n\n    # Preprocess the text data\n    total_words, tokenizer, sentences = preprocess_data(text)\n\n    # Prepare the training data\n    Xs, ys, maxlen = prepare_training_data(sentences, tokenizer, total_words)\n\n    # Initialize an empty dictionary to store model parameters\n    model_parameters = {}\n\n    # Define the list of models to train\n    models = [\n        build_word_model(4, model_parameters, maxlen, total_words),\n        build_word_model(5, model_parameters, maxlen, total_words, d=200, h=150),\n        build_word_model(6, model_parameters, maxlen, total_words, d=300, h=200),\n    ]\n\n    # Train the models and generate text\n    generated_text = fit_word_models(models, Xs, ys, maxlen, tokenizer)\n\n    # Generate the results PDF\n    pdf_components, _ = generate_results_pdf(model_parameters, generated_text)\n\n    # Return the PDF components\n    return pdf_components","metadata":{"id":"i75lluGdXvho","executionInfo":{"status":"ok","timestamp":1714946205919,"user_tz":-180,"elapsed":2,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T09:45:34.926922Z","iopub.execute_input":"2024-05-06T09:45:34.927305Z","iopub.status.idle":"2024-05-06T09:45:34.934786Z","shell.execute_reply.started":"2024-05-06T09:45:34.927275Z","shell.execute_reply":"2024-05-06T09:45:34.933701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pdf_components_2 = train_word_LLMs()","metadata":{"id":"cUBSYC5-pXkG","executionInfo":{"status":"error","timestamp":1714946583095,"user_tz":-180,"elapsed":376656,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"outputId":"26cf0035-30a8-4c39-94a9-9f2c210acb2a","execution":{"iopub.status.busy":"2024-05-06T09:45:38.143781Z","iopub.execute_input":"2024-05-06T09:45:38.144624Z","iopub.status.idle":"2024-05-06T10:41:20.707384Z","shell.execute_reply.started":"2024-05-06T09:45:38.144595Z","shell.execute_reply":"2024-05-06T10:41:20.706404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final conclusion","metadata":{}},{"cell_type":"code","source":"heading = Paragraph(\"Final Conclusion:\")\npara = Paragraph(\"The third Word level Language model is performing very well when generating text and the text seems to be kind of reasonable.\", getSampleStyleSheet()[\"Normal\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-06T10:41:20.709477Z","iopub.execute_input":"2024-05-06T10:41:20.709869Z","iopub.status.idle":"2024-05-06T10:41:20.714974Z","shell.execute_reply.started":"2024-05-06T10:41:20.709813Z","shell.execute_reply":"2024-05-06T10:41:20.713980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outputing the PDF","metadata":{}},{"cell_type":"code","source":"doc.build(pdf_components + pdf_components_2 + [heading] + [Spacer(1, 20)] + [para])","metadata":{"id":"oGYdPgvipgqi","executionInfo":{"status":"aborted","timestamp":1714946583096,"user_tz":-180,"elapsed":7,"user":{"displayName":"Diaa Essam","userId":"00529409558760259485"}},"execution":{"iopub.status.busy":"2024-05-06T10:41:20.716137Z","iopub.execute_input":"2024-05-06T10:41:20.716450Z","iopub.status.idle":"2024-05-06T10:41:21.858002Z","shell.execute_reply.started":"2024-05-06T10:41:20.716406Z","shell.execute_reply":"2024-05-06T10:41:21.857015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}